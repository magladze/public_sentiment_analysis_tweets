# BERT Tweet Sentiment Analysis

This innovative project harnesses the power of BERT to perform sentiment analysis on tweets. BERT, a transformer-based model, is utilized to convert text input into tensors, enabling the prediction of sentiment (POSITIVE or NEGATIVE) for a given sentence. Notably, this process supports sentences in multiple languages.

##  Installation Steps
- Begin by cloning the repository: `https://github.com/magladze/public_sentiment_analysis_twitter`.
- Install `pip` version 20.2 using the command: `pip install pip==20.2`.
- Proceed to install the required dependencies by running: `pip install -r requirements.txt`.
- Download the English language model (large ~382 MB) using: `python -m spacy download en_core_web_lg`.
- Due to computational expenses, consider using an Amazon EC2 instance like `c5a.8xlarge` for optimal performance.
- To access the tweet tensors for the train and test datasets, follow the instructions in `download_tensors.txt` to download and manage the files in a folder named `tensors`.
- Alternatively, you can generate your own tensors using the `embedder_bert_texts.py` script.

## Understanding BERT (Bidirectional Encoder Representations from Transformer)

BERT, like other neural networks, requires text input to be converted into numerical representations to enable processing. This tokenization process involves mapping tokens to integer IDs and adding relevant inputs for model comprehension. The tensors generated by BERT typically have three dimensions:
- **Number of Tweets:** Represents the total number of tweets in the dataset.
- **Sequence Length:** The fixed length of the numerical sequence representing each sentence.
- **Hidden Size:** Denotes the vector dimension of each model input.

The `embedder_bert_texts.py` script transforms tweet inputs into tensors stored in the `data` folder. These tensors, crucial for subsequent analysis, are saved in the `tensors` folder.

## 	Model Training

After converting texts to tensors, various neural networks (NN) are used for training. Models such as Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and a combination of GRU with CNN are evaluated. The training process involves hyperparameter optimization using Random Search.

## Model Evaluation

Following training, the best models are evaluated using the test dataset. Key metrics are saved in the `model_evaluation.csv` file for further analysis. Notable results include accuracy scores for different neural networks and dataset sizes.

## Prediction Time

To predict sentiment for a given sentence, utilize the `predict.py` script. Input your sentence(s) and receive a dictionary output containing the translated English version, sentiment label (POSITIVE or NEGATIVE), and the confidence score associated with the prediction.

For example:
```
>>> python predict.py
Introduce here the sentence(s) you want to predict (separate by | if you introduce more than one):
I'm publishing for the first time on GitHub!

[("Today I'm publishing for the first time on GitHub!", {'to_en': "Today I'm publishing for the first time on GitHub!", 'label': 'POSITIVE', 'score': 0.8692686})]
```

This project showcases the power of BERT in analyzing sentiments expressed in tweets across different languages. 